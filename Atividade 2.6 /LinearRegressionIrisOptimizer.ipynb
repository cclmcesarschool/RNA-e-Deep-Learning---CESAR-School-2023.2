%matplotlib inline
import torch
from torch import nn, optim
from torch.autograd import Variable
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

torch.manual_seed(1234)

iris = load_iris()
data = iris.data[iris.target==1,::2]

x_train = data[:,0:1].astype(np.float32)
y_train = data[:,1:2].astype(np.float32)

n_samples = x_train.shape[0]
print('x_train.shape:',x_train.shape, x_train.dtype)
print('y_train.shape:',y_train.shape, y_train.dtype)

print('x_train[:5]:\n', x_train[:5])
print('y_train[:5]:\n', y_train[:5])


x_train -= x_train.min()
x_train /= x_train.max()
y_train -= y_train.min()
y_train /= y_train.max()

x_train_bias = np.hstack([np.ones(shape=(n_samples,1)), x_train])

x_train_bias = torch.FloatTensor(x_train_bias)
y_train = torch.FloatTensor(y_train)


model = torch.nn.Linear(2, 1, bias=False)

model.weight.data = torch.zeros(1,2)
torch.nn.init.uniform(model.weight.data, -0.1, 0.1)
model.weight.data

model(Variable(torch.ones((5,2))))

criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.3)

num_epochs = 120
w0_list = []
w1_list = []
for epoch in range(num_epochs):
    inputs = Variable(x_train_bias)
    target = Variable(y_train)

    out = model(inputs)

    w0_list.append(model.weight.data[0][0].item())
    w1_list.append(model.weight.data[0][1].item())

    loss = criterion(out, target)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if (epoch+1) % 20 == 0:
       print('Epoch[{}/{}, loss: {:.6f}'
             .format(epoch+1, num_epochs, loss.data))

y_pred = model(Variable(x_train_bias))
plt.plot(x_train, y_train.numpy(),'ro', label='Original data')
plt.plot(x_train, y_pred.data.numpy(), 'kx-', label='Fitting Line' )
plt.show()


Calcule o valor da função de custo (MSE) depois da rede treinada, utilizando a função criterion utilizada no laço de treinamento.

y_pred = model(Variable(x_train_bias))
loss = criterion(y_pred, Variable(y_train))
print(loss)

Faça igual o exercício do notebook anterior, de plotar um gráfico scatterplot para mostrar a evolução dos parâmetros durante o treinamento pelo gradiente descendente.

plt.scatter(w0_list, w1_list)
w0_old = None
for (w0,w1) in zip(w0_list, w1_list):
    if w0_old:
        plt.arrow(w0_old, w1_old, w0-w0_old, w1-w1_old,
                  head_length=0.01,head_width=0.01,shape='full',
                  length_includes_head=True)

